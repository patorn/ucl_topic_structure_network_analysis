{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import os\n",
    "import pdb\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = join('data', 'parsed')\n",
    "OUTPUT_DIR = join('tmp')\n",
    "makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#compile regular expressions that match repeated characters and emoji unicode\n",
    "emoji = re.compile(u'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]',re.UNICODE)\n",
    "multiple = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "\n",
    "def text_format(text):\n",
    "\n",
    "    #strip emoji\n",
    "    stripped = emoji.sub('',text)\n",
    "\n",
    "    #strip URLs\n",
    "    stripped = re.sub(r'http[s]?[^\\s]+','', stripped)\n",
    "\n",
    "    #strip html '&amp;', '&lt;', etc.  \n",
    "    stripped = re.sub(r'[\\&].*;','',stripped)\n",
    "\n",
    "    #strip punctuation\n",
    "    stripped = re.sub(r'[#|\\!|\\-|\\+|:|//]', \" \", stripped)\n",
    "\n",
    "    #strip whitespace down to one.\n",
    "    stripped = re.sub('[\\s]+' ,' ', stripped).strip()\n",
    "\n",
    "    #strip multiple occurrences of letters\n",
    "    stripped = multiple.sub(r\"\\1\\1\", stripped)\n",
    "\n",
    "    #strip all non-latin characters\n",
    "    stripped = re.sub('[^a-zA-Z0-9|\\']', \" \", stripped).strip()\n",
    "\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9095\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "print(\"Building vocabulary...\")\n",
    "\n",
    "docs = {}\n",
    "doc_tokens = {}\n",
    "count = 0\n",
    "\n",
    "for fname in os.listdir(INPUT_DIR):\n",
    "    if fname != '.DS_Store':\n",
    "        with open(join(INPUT_DIR, fname), 'r') as f:\n",
    "            doc = json.load(f)\n",
    "            docs[doc['id']] = doc\n",
    "            text = doc['body']\n",
    "            table = str.maketrans({key: None for key in string.punctuation})\n",
    "            text = text.lower().translate(table)\n",
    "            text = text_format(text)\n",
    "            tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "            words = [word for word,pos in tagged if pos in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "            docs[doc['id']][\"text\"] = \" \".join(words)\n",
    "            clear_output(wait=True)\n",
    "            count += 1\n",
    "            print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9095, 69281)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Processing TF-IDF\")\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(tokenizer=tokenize, norm='l2', stop_words='english')\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(token_values)\n",
    "\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_tfidf, open('results/X_tfidf.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9095, 69281)\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = pickle.load(open('results/X_tfidf.pickle', 'rb'))\n",
    "\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Keywords to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, key in enumerate(token_keys):\n",
    "    print('{0}\\r'.format(i/len(token_keys)))\n",
    "    clear_output(wait=True)\n",
    "    docs[key]['top_tfidf'] = top_feats_in_doc(X_tfidf, vectorizer_tfidf.get_feature_names(), i, 50)\n",
    "    \n",
    "print(docs['2016-05-20-28']['top_tfidf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Vector to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x69281 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 196 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, key in enumerate(token_keys):\n",
    "    docs[key]['tfidf'] = X_tfidf[i]\n",
    "\n",
    "docs['2016-05-20-28']['tfidf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(docs, open('results/docs.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['authors', 'section_id', 'id', 'top_tfidf', 'mod_topic', 'webUrl', 'tags', 'webTitle', 'keyword', 'guardianId', 'apiUrl', 'webPublicationDate', 'sectionId', 'body', 'tfidf', 'text'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pickle.load(open('results/docs.pickle', 'rb'))\n",
    "\n",
    "docs['2016-05-20-28'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9095\n"
     ]
    }
   ],
   "source": [
    "sorted_docs = sorted(docs.items(), key=lambda x: x[0], reverse=True)\n",
    "token_keys = [x[0] for x in sorted_docs]\n",
    "token_values = [x[1][\"text\"] for x in sorted_docs]\n",
    "print(len(token_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network-based Approach - Build the graph from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = ig.Graph()\n",
    "for i, key in enumerate(token_keys):\n",
    "    g.add_vertices(key)\n",
    "    \n",
    "print(g.vcount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.delete_edges(g.es)\n",
    "edges = []\n",
    "for i, source in enumerate(token_keys):\n",
    "    clear_output(wait=True)\n",
    "    print('{0}\\r'.format(i/len(token_keys)))\n",
    "    for j, target in enumerate(token_keys):\n",
    "        edges.append((source, target))\n",
    "            \n",
    "g.add_edges(edges)  \n",
    "edges = [] \n",
    "print(g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "count = 0\n",
    "cos_sims = cosine_similarity(X_tfidf)\n",
    "for i, source in enumerate(token_keys):\n",
    "    sims = cos_sims[i]\n",
    "    print('{0}\\r'.format(i/len(token_keys)))\n",
    "    clear_output(wait=True)\n",
    "    for j, target in enumerate(token_keys):            \n",
    "        g.es[count][\"weight\"] = sims[j]\n",
    "        count += 1\n",
    "\n",
    "print(g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.simplify(multiple=True, loops=True, combine_edges=\"max\")\n",
    "print(g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.es[1]['weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Sub Graph or Use below function to load from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_g = g.es.select(weight_ge=0.2).subgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_g.write_gml('results/graph_02.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_g = ig.Graph.Read_GML('results/graph_02.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_properties(g, community):\n",
    "    print(\"Nodes {}\".format(g.vcount()))\n",
    "    print(\"Edges {}\".format(g.ecount()))\n",
    "    print(\"Diameter {}\".format(g.diameter()))\n",
    "    print(\"LCC {}\".format(g.clusters().giant().vcount()))\n",
    "    print(\"APL {}\".format(g.average_path_length()))\n",
    "    print(\"AD {}\".format(sum(g.degree())/len(g.degree())))\n",
    "    print(\"Giant Size {}\".format(len(community.giant().vs())))\n",
    "    print(\"NO of Communities {}\".format(max(community.membership) + 1))\n",
    "    print(\"Modularity {}\".format(g.modularity(community.membership)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586\n",
      "166\n",
      "140\n",
      "139\n",
      "Nodes 8213\n",
      "Edges 132243\n",
      "LCC 7950\n",
      "NO of Communities 139\n",
      "Modularity 0.7987487477056004\n"
     ]
    }
   ],
   "source": [
    "sg = sub_g.es.select(weight_ge=0.2).subgraph()\n",
    "communities = sg.community_multilevel(weights='weight',return_levels=True)\n",
    "for com in communities:\n",
    "    print(max(com.membership) + 1)\n",
    "community = communities[-1]\n",
    "print(\"Nodes {}\".format(sg.vcount()))\n",
    "print(\"Edges {}\".format(sg.ecount()))\n",
    "print(\"LCC {}\".format(sg.clusters().giant().vcount()))\n",
    "print(\"NO of Communities {}\".format(max(community.membership) + 1))\n",
    "print(\"Modularity {}\".format(sg.modularity(community.membership)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network_to_topics(com):\n",
    "    topics = {}\n",
    "\n",
    "    for i in range(max(com.membership) + 1):\n",
    "        for v in com.subgraph(i).vs():\n",
    "            topic_no = i\n",
    "            try:\n",
    "                topics[topic_no]\n",
    "            except KeyError:\n",
    "                topics[topic_no] = []\n",
    "\n",
    "            topics[topic_no].append(v['name'])\n",
    "    \n",
    "    net_topics = {}\n",
    "    count = 1\n",
    "    for key, value in sorted(topics.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        net_topics[count] = value\n",
    "        count += 1\n",
    "        \n",
    "    return net_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_topics = network_to_topics(community)\n",
    "len(net_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "def keywords_to_keyword_table(topics, n):\n",
    "    result = {}\n",
    "    for topic in topics:\n",
    "        keywords = []\n",
    "        for top in topic[1]:\n",
    "            keywords += list(top['feature'])\n",
    "        net_keywords = Counter(keywords)\n",
    "        net_keywords = sorted(net_keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "        keywords2 = list(map(lambda x: x[0], net_keywords))\n",
    "        result[topic[0]] = (len(topic[1]), keywords2[:n])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topics_to_keywords(topics):\n",
    "    results = {}\n",
    "\n",
    "    for k, vs in topics.items():\n",
    "        results[k] = []\n",
    "        for v in vs:\n",
    "            top_df = docs[v]['top_tfidf']\n",
    "            results[k].append(top_df)\n",
    "#             results[k].append(top_df[(top_df.tfidf >= 0.02)])\n",
    "\n",
    "    results = sorted(results.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_keywords = keywords_to_keyword_table(topics_to_keywords(net_topics), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "net_s_k = sorted(net_keywords.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "\n",
    "length = len(net_s_k)\n",
    "rows = []\n",
    "for i in range(length):\n",
    "    row = [i+1]\n",
    "    row += [net_s_k[i][1][0], net_s_k[i][1][1]]\n",
    "    rows.append(row)\n",
    "\n",
    "with open('tmp/keywords.csv', 'w+') as file:    \n",
    "    for row in rows:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "g = ig.Graph()\n",
    "for i in range(30):\n",
    "    g.add_vertices(str(i + 1))\n",
    "    \n",
    "print(g.vcount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for i in range(30):\n",
    "    labels.append(\",\".join(net_keywords[i + 1][1][:5]))\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "g.delete_edges(g.es)\n",
    "edges = []\n",
    "for i in range(30):\n",
    "    source = str(i+1)\n",
    "    for j in range(30):\n",
    "        target = str(j+1)\n",
    "        if source != target:\n",
    "            dup = set(net_keywords[i + 1][1]) & set(net_keywords[j + 1][1])\n",
    "            if dup:\n",
    "                g.add_edge(source,target,weight=len(dup)) \n",
    "\n",
    "print(g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "g.simplify(multiple=True, loops=True, combine_edges=\"max\")\n",
    "print(g.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.write_gml('tmp/words.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.wrappers import LdaVowpalWabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9095, 69281)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Processing Count\")\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "X_count = vectorizer.fit_transform(token_values)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "print(X_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_count, open('results/X_count.pickle', 'wb'))\n",
    "pickle.dump(vocab, open('results/X_count_vocab.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9095, 69281)\n"
     ]
    }
   ],
   "source": [
    "X_count = pickle.load(open('results/X_count.pickle', 'rb'))\n",
    "vocab = pickle.load(open('results/X_count_vocab.pickle', 'rb'))\n",
    "\n",
    "print(X_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for the best number of topic K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_to_folds(data, k):\n",
    "  folds = []\n",
    "\n",
    "  for i in range(k):\n",
    "    fold = []\n",
    "    for j in range(i, data.shape[0], k):\n",
    "      fold.append(data[j])\n",
    "    folds.append(np.random.permutation(fold))\n",
    "\n",
    "  return folds\n",
    "\n",
    "folds = data_to_folds(X_count, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition data to train and test sets\n",
    "# return list train data, list of test data\n",
    "def split_folds(folds, i):\n",
    "  train = folds[:i] + folds[i+1:]\n",
    "  train = [x for data in train for x in data] # Flatten\n",
    "  train = scipy.sparse.vstack((train))\n",
    "  test = scipy.sparse.vstack(folds[i])\n",
    "  \n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_count_train, X_count_test = split_folds(folds, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = LdaVowpalWabbit('/usr/local/Cellar/vowpal-wabbit/8.1.1/bin/vw',\n",
    "                     corpus=matutils.Sparse2Corpus(X_count_train.T),\n",
    "                     num_topics=num_topics,\n",
    "                     alpha=(1/num_topics),\n",
    "                     eta=(1/num_topics),\n",
    "                     passes=50,\n",
    "                     id2word=dict([(i, s) for i, s in enumerate(vocab)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.258062\n",
      "-8.605109\n",
      "3858.60886565\n",
      "5459.48094936\n"
     ]
    }
   ],
   "source": [
    "train_log_prep_gensim = lda_model.log_perplexity(matutils.Sparse2Corpus(X_count_train.T))\n",
    "test_log_prep_gensim = lda_model.log_perplexity(matutils.Sparse2Corpus(X_count_test.T))\n",
    "print(train_log_prep_gensim)\n",
    "print(test_log_prep_gensim)\n",
    "print(np.exp(-train_log_prep_gensim))\n",
    "print(np.exp(-test_log_prep_gensim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LdaVowpalWabbit('/usr/local/Cellar/vowpal-wabbit/8.1.1/bin/vw',\n",
    "                     corpus=matutils.Sparse2Corpus(X_count.T),\n",
    "                     num_topics=num_topics,\n",
    "                     alpha=(1/num_topics),\n",
    "                     eta=(1/num_topics),\n",
    "                     passes=100,\n",
    "                     id2word=dict([(i, s) for i, s in enumerate(vocab)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model.save('results/lda_' + str(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = LdaVowpalWabbit.load('results/lda_' + str(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_matrix = lda_model.show_topics(formatted=False, num_words=10, num_topics=lda_model.num_topics)\n",
    "lda_topic_keywords = {}\n",
    "count = 1\n",
    "for topic in topics_matrix:\n",
    "    lda_topic_keywords[count] = [str(word[1]) for word in topic]\n",
    "    count += 1\n",
    "\n",
    "lda_topic_keywords.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "lda_s_k = sorted(lda_topic_keywords.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "length = len(lda_s_k)\n",
    "rows = []\n",
    "for i in range(length):\n",
    "    row = [i+1]\n",
    "    row += [lda_s_k[i][0], lda_s_k[i][1]]\n",
    "    rows.append(row)\n",
    "\n",
    "with open('tmp/keywords.csv', 'w+') as file:    \n",
    "    for row in rows:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Coherence Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [token for token in nltk.word_tokenize(text)]\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "count = 0\n",
    "for value in token_values:\n",
    "    texts.append(tokenize(value))\n",
    "    clear_output(wait=True)\n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_words = [['yorkshir', 'detent', 'applic', 'brain', 'ali'],\n",
    "['year', 'govern', 'servic', 'plan', 'council'],\n",
    "['year', 'time', 'photograph', 'day', 'thing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = CoherenceModel(topics=topic_words, \n",
    "                    corpus=corpus, \n",
    "                    dictionary=dictionary, \n",
    "                    coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm.get_coherence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
